---
title: "Assignment 1"
author: "Junbin Wu", "SO Ji Yue"
date: "2024-03-10"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Content

1. Model functions (in chronological order of exploration)
  1.1 Simple Linear Model
  1.2 All zeros
  1.3 Decision Tree
  1.4 PCA
    1.4.1 PCA original
    1.4.2 PCA with cross validation
  
2. Model Evaluation functions
  2.1 MSE calculation

3. Main functions
  3.1 Library loading
  3.2 Data loading
  3.3 Models Running
    3.3.1 Simple linear model
    3.3.2 All zeros
    3.3.3 Decision tree
      3.3.3.1 Decision tree using the whole set
      3.3.3.2 Decision second try
    3.3.4 PCA
      3.3.4.1 PCA first try
      3.3.4.2 PCA cross validation
      3.3.4.3 PCA third try
    3.3.5 Neural network
      3.3.5.1 Neural Network firs try using neuralnet
      3.3.5.2 Neural network using tensorflow and keras 
      3.3.5.3 Neural Network for subset's outcome between 0 and 75
      3.3.5.4 Neural network with independent variables processed by PCA
      3.3.5.5 Categorical neural network 286 classes
      3.3.5.6 Categorical neural network 1425 classes
    3.3.6 Gradient boosting
    3.3.7 Naive Bayes Gaussian
    3.3.8 Random forest
  3.4 Model evaluation
  3.5 Output as a CSV

# 1. Model functions (in chronological order)

## 1.1 Simple Linear Model

input a training data set
output fit model
```{r}
sim_liner_mod <- function(train) {
  # use `52` as the independent varaible, and `281` as the dependent variable.
  fit <- lm(C281 ~ C7 + C52 + C61 + C62, data = train)
  return(fit)
}
```

## 1.2 All zeros
By predicting all outcomes as zeros, the MSE is even better than 2.1 Linear Model, which indicate that a simple linear regression would not work.

## 1.3 Decision tree
input a training data set
output a fit model
```{r}

deci_tree <- function(train) {
  tree <- rpart(C281 ~ ., data = train, method = "anova")
  return(tree)
}

```

## 1.4 PCA

### 1.4.1 PCA original
input two data set, the train and the test
output predictions on the test
```{r}
PCA <- function(train, test) {
  
#####train the model
  # independent_variables
  independent_vars <- train[, !names(train) %in% "C281"]
  
  # Scale the independent variables
  independent_vars_scaled <- scale(independent_vars)
  
  # Running PCA on the independent variables
  pca_result <- prcomp(independent_vars_scaled, scale. = TRUE)
  
  # Scree plot to aid in selecting the number of principal components
  #plot(pca_result$sdev^2, type = "b", xlab = "Principal Component", ylab = "Variance Explained")
  
  # calculate the explained variances
  prop_var_explained <- cumsum(pca_result$sdev^2) / sum(pca_result$sdev^2)
  
  # only select whose explained variances larger than 0.95
  n_components <- which.max(prop_var_explained >= 0.95)
  
  pca_scores <- pca_result$x[, 1:n_components]
  
  # Combine PCA scores with the outcome variable
  regression_data <- as.data.frame(cbind(pca_scores, outcome = train$C281))
  
  # Linear regression
  lm_result <- lm(outcome ~ ., data = regression_data)
  

##### make predictions
  #  test is the new test dataset
  new_independent_vars <- test[, names(test) %in% names(independent_vars)]
  # Scale the test
  new_independent_vars_scaled <- scale(new_independent_vars, center = attr(independent_vars_scaled, "scaled:center"), scale = attr(independent_vars_scaled, "scaled:scale"))
  
  # Transform test data with PCA
  new_pca_scores <- predict(pca_result, newdata = new_independent_vars_scaled)
  
  # Selecting the same number of principal components
  new_pca_scores_selected <- new_pca_scores[, 1:n_components]
  
  # Make predictions
  predictions <- predict(lm_result, newdata = data.frame(new_pca_scores_selected))
  return(predictions)
}
```

### 1.4.2 PCA with cross validation
input training set, seed for randomization, k-fold of cross validation
output the cross validation results
```{r}
PCAcv <- function(train, seed, kf) {
  
  # outcome variable
  outcome_var <- train$C281
  
  # independent variables
  independent_vars <- train[, !names(train) %in% "C281"]
  # Scale the independent variables
  independent_vars_scaled <- scale(independent_vars)
  
  #set cv
  set.seed(seed) # for reproducibility

  folds <- createFolds(outcome_var, k = kf, list = TRUE)
  


  # Placeholder for cross-validation results
  cv_results <- data.frame(RMSE = rep(NA, length(folds)))


  # Loop over each fold
  for(i in seq_along(folds)) {
      # Split the data into training and testing sets
      test_indices <-  folds[[i]]
      train_indices <- (1:nrow(train))[-test_indices]
      
      
      # PCA on training data
      pca_result <- prcomp(independent_vars_scaled[train_indices, ], center = TRUE, scale. = TRUE)
      # Determine number of components explaining 95% variance or use another criterion
      prop_var_explained <- cumsum(pca_result$sdev^2) / sum(pca_result$sdev^2)
      num_components <- which.max(prop_var_explained >= 0.95)
      # Prepare the training data with selected components
      pca_train <- predict(pca_result, newdata = independent_vars_scaled[train_indices, ])[, 1:num_components]
      pca_test <- predict(pca_result, newdata = independent_vars_scaled[test_indices, ])[, 1:num_components]
      
      model <- glmnet(pca_train, outcome_var[train_indices], family = "gaussian")
      summary(model)
      # Predict on testing data
      predictions <- predict(model, newx = pca_test)
      
      # Calculate performance metrics
      cv_results$RMSE[i] <- sqrt(mean((predictions - outcome_var[test_indices])^2))
      #cv_results$R2[i] <- cor(predictions, outcome_var[test_indices])^2
  }
  return(cv_results)

}
```

### 1.4.3 PCA and Decision tree
input two data set, the train and the test
output predictions on the test
```{r}
PCA_Deci_tree <- function(train, test) {
  
#####train the model
  # independent_variables
  independent_vars <- train[, !names(train) %in% "C281"]
  
  # Scale the independent variables
  independent_vars_scaled <- scale(independent_vars)
  
  # Running PCA on the independent variables
  pca_result <- prcomp(independent_vars_scaled, scale. = TRUE)
  
  # Scree plot to aid in selecting the number of principal components
  #plot(pca_result$sdev^2, type = "b", xlab = "Principal Component", ylab = "Variance Explained")
  
  # calculate the explained variances
  prop_var_explained <- cumsum(pca_result$sdev^2) / sum(pca_result$sdev^2)
  
  # only select whose explained variances larger than 0.95
  n_components <- which.max(prop_var_explained >= 0.99)
  
  pca_scores <- pca_result$x[, 1:n_components]
  
  # Combine PCA scores with the outcome variable
  regression_data <- as.data.frame(cbind(pca_scores, C281 = train$C281))
  
  # Linear regression
  model <- deci_tree(regression_data)
  

##### make predictions
  #  test is the new test dataset
  new_independent_vars <- test[, names(test) %in% names(independent_vars)]
  # Scale the test
  new_independent_vars_scaled <- scale(new_independent_vars, center = attr(independent_vars_scaled, "scaled:center"), scale = attr(independent_vars_scaled, "scaled:scale"))
  
  # Transform test data with PCA
  new_pca_scores <- predict(pca_result, newdata = new_independent_vars_scaled)
  
  # Selecting the same number of principal components
  new_pca_scores_selected <- new_pca_scores[, 1:n_components]
  
  # Make predictions
  predictions <- predict(model, newdata = data.frame(new_pca_scores_selected))
  return(predictions)
}
```

# 2. Model Evaluation functions

## 2.1 MSE calculation
input a vector of prediction value, the test data set
output a mse
```{r}
MSE <- function(prediction, test) {
  mse <- mean((prediction - test$C281)^2)
  return(mse)
}

```




# 3. Main functions
The Main function calls all other functions above intermittently for the purpose of high efficiency and maintenance.



## 3.1 Library loading
```{r}
library(readr)
library(glmnet)
library(rpart)
library(rpart.plot)
library(rlang)
library(dplyr)
library(ggplot2)
library(cli)
library(caret)
library(keras)
library(mlbench)
library(magrittr)
library(neuralnet)
library(Formula)
library(tensorflow)
library(reticulate)
library(e1071)
library(data.table)
library(xgboost)
library(caTools) 
library(randomForest) 
```



## 3.2 Data loading
The data used here have been pre-processed. I added headers 1-281 for every data set. For the training set, I removed all duplicates, so the total observations come down from 52,397 to 49,203.
```{r message=FALSE, warning=FALSE}
train_set <- read_csv("data/Processed Data Set/blogData_train duplicate removed.csv")

final_test_set <- read_csv("data/Processed Data Set/blogData_test.csv")

test_set_201 <- read_csv("data/Processed Data Set/blogData_test-2012.02.01.00_00.csv")

test_set_202 <- read_csv("data/Processed Data Set/blogData_test-2012.02.02.00_00.csv")

test_set_203 <- read_csv("data/Processed Data Set/blogData_test-2012.02.03.00_00.csv")
```


## 3.3 Models running

### 3.3.1 Simple Linear Model
```{r}
fit <- sim_liner_mod(train_set)
predictions <- predict(fit, final_test_set)
```

### 3.3.2: the results from 3.3.1 are so bad that it is even worse than entering all zeros 

### 3.3.3 Decision tree
#### 3.3.3.1 decision tree first try
```{r}
first_tree <- deci_tree(train_set)
predictions <- predict(first_tree, final_test_set, type = "matrix")


```

### 3.3.3.2 decision tree second try, training specifically for whose output probably >3 and < 200
```{r}

# the first training model 
tree <- deci_tree(train_set)


# identify those who are significantly larger outlier
predictions <- predict(tree, final_test_set,type = "matrix")
thres <- quantile(predictions)[[4]] # set the threshold of filtering
predictions.df <- as.data.frame(cbind(ID = c(0:213),predictions))
predictions.df <- predictions.df[which(predictions.df$predictions > thres), ]
id <- predictions.df$ID
final_test_set3plus <- final_test_set[id,]

# the second training model 
train_3plus <- train_set[which(train_set$C281 >3 & which(train_set$C281<200)),]
tree3plus <- deci_tree(train_3plus)



# predict
predictions <- predict(tree3plus, final_test_set3plus,type = "matrix")
```


### 3.3.4 PCA

#### 3.3.4.1 PCA first try
```{r}
# remove any constant zeros column
train_set_zeroCOLremoved <- train_set[,apply(train_set, 2, function(x) var(x)!=0)]

predictions <- cbind(ID = c(0:213), num_comments = PCA(train_set_zeroCOLremoved, final_test_set))
```


#### 3.3.4.2 PCA second try, using cross-validation
```{r}
train_set_zeroCOLremoved <- train_set[,apply(train_set, 2, function(x) var(x)!=0)]
cvresults <- PCAcv(train_set_zeroCOLremoved, 123, 5)

```


#### 3.3.4.3 PCA third try, identify potential outliers and train a extra model for those outliers
```{r}
#train_set_zeroCOLremovedPlus10 <- train_set[,]
# for this try in PCA, I try a model specifically fo outliers.
train_set_zeroCOLremoved <- train_set[,apply(train_set, 2, function(x) var(x)!=0)]
predictions <- as.data.frame(cbind(ID = c(0:213), num_comments = PCA(train_set_zeroCOLremoved,final_test_set)))

thres <- quantile(predictions$num_comments)[[4]] # set the threshold of filtering
predictions.df <- predictions[which(predictions$num_comments > thres), ]
id <- predictions.df$ID

# only use data whose output is between 2 and 200
train_set_zeroCOLremoved <- train_set[which(train_set$C281 > 2 & train_set$C281< 200),]
train_set_zeroCOLremoved <- train_set_zeroCOLremoved[,apply(train_set_zeroCOLremoved, 2, function(x) var(x)!=0)]

id <- id+1

final_test_set2plus <- final_test_set[id,]

predictions <- cbind(ID = id-1, num_comments = PCA(train_set_zeroCOLremoved,final_test_set2plus))

```
#### 3.3.4.4 Fourth try on PCA: try 5 different PCA for different types of results
This does not work well.
```{r}
#using 3.3.4.4, I identify 4 classes of output, and classify them into several id lists
# i train for each class individually
#train_set_no_word <- train_set[,-(63:262)]
#train_set_no_word <- train_set_no_word[,-(1:50)]
train_set_no_word <- train_set[which(train_set$C281 > 3&train_set$C281 < 20),]
train_set_zeroCOLremoved <- train_set_no_word[,apply(train_set_no_word, 2, function(x) var(x)!=0)]
id <- c(4,6,8,16,20,22,24,25,29,30,38,40,67,71,78,90,93,95,103,107,109,111,139,141,150,153,155,159,161,166,167,189,201,208)
id <- id + 1
final_test_set2plus <- final_test_set[id,]

ID200plus <- c(131) + 1
ID50_200 <- c(20,30,208) +1 
ID20_50 <- c(40,71,95,109,111,141,153,155,167,189,201) +1
ID3_20 <- c(4,6,8,16,22,29,38,67,78,90,93,103,107,139,150,159,161)+1

predictions <- PCA_Deci_tree(train_set_zeroCOLremoved, final_test_set[ID3_20,])


predictions <- PCA_Deci_tree(train_set_zeroCOLremoved, final_test_set2plus)
predictions
MSE(predictions, test_set_203)

```

#### 3.3.4.5 Fivth try on PCA: scale on both input and output
```{r}
#train_set_no_word <- train_set[which(train_set$C281 > 1 & train_set$C281 < 100),]

train_set_5plus <- train_set[which(train_set$C281 > 0),]
train_set_5plus <- train_set_5plus[,apply(train_set_5plus, 2, function(x) var(x)!=0)]

hist(train_set_5plus$C281)

train_set_5plus$C281 <- log(train_set_5plus$C281)
hist(train_set_5plus$C281)


predictions <- as.data.frame(cbind(ID = c(0:213), num_comments = PCA(train_set_5plus, final_test_set)))


predictions$num_comments <- exp(predictions$num_comments)
predictions


```
### 3.3.5 Neural network

#### 3.3hist(train_set[,1]).5.1 Neural Network firs try using neuralnet 
```{r}

####first try
train_set_zeroCOLremoved <- train_set[which(train_set$C281 > 2 & train_set$C281< 200),]
train_set_zeroCOLremoved <- train_set_zeroCOLremoved[,apply(train_set_zeroCOLremoved, 2, function(x) var(x)!=0)]
  
# C281 is the output variable
  C281 <- train_set_zeroCOLremoved$C281
  independent_vars <- train_set_zeroCOLremoved[, !names(train_set_zeroCOLremoved) %in% "C281"]
# Scale the independent variables
  independent_vars_scaled <- scale(independent_vars)
# scale train set
  scale_train <- data.frame(cbind(independent_vars_scaled, C281))

  num_classes <- length(unique(C281))

# build the model
n1 <- neuralnet(C281 ~ C51+C52+C53+C54+C55+C56+C57+C58+C59+C60+C61+C62,  
               data = scale_train,
               hidden = c(5),
               linear.output = F,
               threshold = 0.1, 
               stepmax = 10000,
               rep=2)
#plot the model
# plot(n1,col.hidden = 'darkgreen',     
# col.hidden.synapse = 'darkgreen',
#      show.weights = F,
#      information = F,
#      fill = 'lightblue')

predictions <- predict(n1, test_set_201)
predictions
```


#### 3.3.5.2 Neural network using tensorflow and keras 
```{r}
train_set <- read_csv("data/Processed Data Set/blogData_train duplicate removed.csv")
#hist(train_set[,1:50])
#train_set_5plus <- train_set[which(train_set$C281 > 5),]
#train_set_5plus <- train_set[,apply(train_set, 2, function(x) var(x)!=0)]

# set randomization
set.seed(354)

# set training and test set index
split_index <- sample(1:nrow(train_set), size = floor(0.8 * nrow(train_set)))

train_data <- train_set[split_index, ]  # 80% of data for training

test_data <- train_set[-split_index, ]  # Remaining 20% for testing

#dataframe with 280 features and 1 output variable

input_data_train <- as.matrix(train_data[, !names(train_data) %in% "C281"])  # Selecting the first 280 columns as features
output_data_train <- as.matrix(train_data[, names(train_data) %in% "C281"]) # Assuming the output variable is in the 281st column
input_data_test <- as.matrix(test_data[, !names(train_data) %in% "C281"])
output_data_test <- as.matrix(test_data[, names(train_data) %in% "C281"])

# the neural network model
model <- keras_model_sequential() %>%
  layer_dense(units = 180, activation = 'relu',input_shape = c(280)) %>%
  layer_dense(units = 180, activation = 'relu') %>%
  layer_dense(units = 180, activation = 'relu') %>%
  layer_dense(units = 1)



# Compile the model with Mean Squared Error as the loss function and an optimizer
model %>% compile(
  loss = 'mean_absolute_error',  # Using MSE as the loss function
  optimizer = "adam",  
  metrics = 'accuracy'  # Accuracy can still be useful to see 
)



# Step 5: Train the Model
history <- model %>% fit(
  input_data_train, output_data_train,
  epochs = 100,
  batch_size = 15,
  validation_split = 0.1
)


model %>% evaluate(input_data_test, output_data_test)

# identify 0 or 1 first
new_data_matrix <- as.matrix(final_test_set[,1:280])
predictions <- predict(model, new_data_matrix)
predictions

MSE(predictions, test_set_203)

MSE(predictions, test_set_203)

```


#### 3.3.5.3 Neural Network for subset's outcome between 0 and 75
```{r}

train_set_0plus <- train_set[which(train_set$C281 > 0 & train_set$C281 < 75 ),]

# set randomization
set.seed(354)
split_index <- sample(1:nrow(train_set_0plus), size = floor(0.8 * nrow(train_set_0plus)))

train_data <- train_set_0plus[split_index, ]  # 80% of data for training

test_data <- train_set_0plus[-split_index, ]  # Remaining 20% for testing

#dataframe with 280 features and 1 output variable

input_data_train <- as.matrix(train_set_0plus[, !names(train_data) %in% "C281"])  # Selecting the first 280 columns as features
output_data_train <- as.matrix(train_set_0plus[, names(train_data) %in% "C281"]) # Assuming the output variable is in the 281st column
input_data_test <- as.matrix(train_set_0plus[, !names(train_data) %in% "C281"])
output_data_test <- as.matrix(train_set_0plus[, names(train_data) %in% "C281"])

# the neural network
model <- keras_model_sequential() %>%
  layer_dense(units = 180, activation = 'relu',input_shape = c(280)) %>%
  layer_dense(units = 180, activation = 'relu') %>%
  layer_dense(units = 180, activation = 'relu') %>%
  layer_dense(units = 1)



# Compile the model with Mean Squared Error as the loss function and an optimizer
model %>% compile(
  loss = 'mean_absolute_error',  # Using MAE as the loss function
  optimizer = "adamax",  # I have tried different optimizer
  metrics = 'accuracy'  # Accuracy can still be useful to see 
)



# Step 5: Train the Model
history <- model %>% fit(
  input_data_train, output_data_train,
  epochs = 100,
  batch_size = 30,
  validation_split = 0.1
)



model %>% evaluate(input_data_test, output_data_test)


new_data_matrix <- as.matrix(final_test_set[,1:280])
predictions <- predict(model, new_data_matrix)


```



#### 3.3.5.4 Neural network with independent variables processed by PCA
```{r}
#### 3.3.5.2 Neural network using tensorflow and keras 
#train_set_5plus <- train_set[which(train_set$C281 > 5),]
#train_set_5plus <- train_set[,apply(train_set, 2, function(x) var(x)!=0)]

######################################################################

# remove 0 variance col in train data for PCA
train_set <- train_set[,apply(train_set, 2, function(x) var(x)!=0)]

# set randomization
set.seed(213)
split_index <- sample(1:nrow(train_set), size = floor(0.8 * nrow(train_set)))

train_data <- train_set[split_index, ]  # 80% of data for training
test_data <- train_set[-split_index, ]  # Remaining 20% for testing


#dataframe with 280 features and 1 output variable
input_data_train <- as.matrix(train_data[, !names(train_data) %in% "C281"])  # Selecting the first 280 columns as features
output_data_train <- as.matrix(train_data[, names(train_data) %in% "C281"]) # Assuming the output variable is in the 281st column
input_data_test <- as.matrix(test_data[, !names(train_data) %in% "C281"])
output_data_test <- as.matrix(test_data[, names(train_data) %in% "C281"])
#####train the model


  
  # Scale the independent variables
  independent_vars_scaled <- scale(input_data_train)
  
  # Running PCA on the independent variables
  pca_result <- prcomp(independent_vars_scaled, scale. = TRUE)
  
  # Scree plot to aid in selecting the number of principal components
  #plot(pca_result$sdev^2, type = "b", xlab = "Principal Component", ylab = "Variance Explained")
  
  # calculate the explained variances
  prop_var_explained <- cumsum(pca_result$sdev^2) / sum(pca_result$sdev^2)
  
  # only select whose explained variances larger than 0.95
  n_components <- which.max(prop_var_explained >= 0.95)
  
  pca_scores <- as.matrix(pca_result$x[, 1:n_components])
  

####

  # Scale the test
  new_independent_vars_scaled <- scale(input_data_test, center = attr(independent_vars_scaled, "scaled:center"), scale = attr(independent_vars_scaled, "scaled:scale"))
  
  # Transform test data with PCA
  new_pca_scores <- predict(pca_result, newdata = new_independent_vars_scaled)
  
  # Selecting the same number of principal components
  new_pca_scores_selected <- new_pca_scores[, 1:n_components]
  

#######################################################################



# neural network model
model <- keras_model_sequential() %>%
  layer_dense(units = 89, activation = 'relu',input_shape = c(182)) %>%
  layer_dense(units = 180, activation = 'relu') %>%
  layer_dense(units = 180, activation = 'relu') %>%
  layer_dense(units = 180, activation = 'relu') %>%
  layer_dense(units = 180, activation = 'relu') %>%
  layer_dense(units = 1)



# Compile the model with Mean Squared Error as the loss function and an optimizer
model %>% compile(
  loss = 'mean_squared_error',  # Using MSE as the loss function
  optimizer = "adam",  
  metrics = 'accuracy'  # Accuracy can still be useful to see 
)



# Step 5: Train the Model
history <- model %>% fit(
  pca_scores, output_data_train,
  epochs = 150,
  batch_size = 30,
  validation_split = 0.2
)



model %>% evaluate(new_pca_scores_selected, output_data_test)



new_data_matrix <- as.matrix(final_test_set[,1:280])
predictions <- predict(model, new_data_matrix)
predictions

MSE(predictions, test_set_203)

```


#### 3.3.5.5 Categorical neural network 286 classes
```{r}



train_set <- read_csv("data/Processed Data Set/blogData_train duplicate removed.csv")


# need one-hot encoding here

for (i in 282:567) {
  train_set[,i] <- 0
}

for (i in 1:49203){
  this.column <- as.numeric(floor(train_set[i,281] /5))
  train_set[i,281+this.column+1] <- 1
  this.column <-0
}


# randomization
set.seed(354)
split_index <- sample(1:nrow(train_set), size = floor(0.8 * nrow(train_set)))

train_data <- train_set[split_index, ]  # 80% of data for training

test_data <- train_set[-split_index, ]  # Remaining 20% for testing

#dataframe with 280 features and 1 output variable

input_data_train <- as.matrix(train_data[, 1:280])  # Selecting the first 280 columns as features
output_data_train <- as.matrix(train_data[, 282:567]) # Assuming the output variable is in the 281st column
input_data_test <- as.matrix(test_data[, 1:280])
output_data_test <- as.matrix(test_data[, 282:567])

# the neural network model
model <- keras_model_sequential() %>%
  layer_dense(units = 180, activation = 'relu',input_shape = c(280)) %>%
  layer_dense(units = 180, activation = 'relu') %>%
  layer_dense(units = 180, activation = 'relu') %>%
  layer_dense(units = 286, activation = 'softmax')


# Compile the model 
model %>% compile(
  loss = 'categorical_crossentropy',  
  optimizer = "adam",  
  metrics = 'accuracy'  # Accuracy can still be useful to see 
)



# Step 5: Train the Model
history <- model %>% fit(
  input_data_train, output_data_train,
  epochs = 10,
  batch_size = 30,
  validation_split = 0.1
)

model %>% evaluate(input_data_test, output_data_test)



predictions <- predict(model, as.matrix(final_test_set[,-281]))

predictions.df <- as.data.frame(predictions)
predictions.df$num <- 0

# estimate the predictions by using each probability of each category * the mean of that category
for (i in 1:214) {
  for (j in 1:286) {
    predictions.df[i,287] <- predictions.df[i,287] + predictions.df[i,j]*(j-0.5)*5
  }
}
predictions.df[,287]
```

#### 3.3.5.6 Categorical neural network 1425 classes

```{r}

train_set <- read_csv("data/Processed Data Set/blogData_train duplicate removed.csv")


# need one-hot encoding here

for (i in 282:1706) {
  train_set[,i] <- 0
}

for (i in 1:49203){
  this.column <- as.numeric(train_set[i,281])
  train_set[i,281+this.column+1] <- 1
  this.column <-0
}


# randomization
set.seed(354)
split_index <- sample(1:nrow(train_set), size = floor(0.8 * nrow(train_set)))

train_data <- train_set[split_index, ]  # 80% of data for training

test_data <- train_set[-split_index, ]  # Remaining 20% for testing

#dataframe with 280 features and 1 output variable

input_data_train <- as.matrix(train_data[, 1:280])  # Selecting the first 280 columns as features
output_data_train <- as.matrix(train_data[, 282:1706]) # Assuming the output variable is in the 281st column
input_data_test <- as.matrix(test_data[, 1:280])
output_data_test <- as.matrix(test_data[, 282:1706])

# the neural network model
model <- keras_model_sequential() %>%
  layer_dense(units = 180, activation = 'relu',input_shape = c(280)) %>%
  layer_dense(units = 180, activation = 'relu') %>%
  layer_dense(units = 180, activation = 'relu') %>%
  layer_dense(units = 1425, activation = 'softmax')


# Compile the model 
model %>% compile(
  loss = 'categorical_crossentropy',  
  optimizer = "adam",  
  metrics = 'accuracy'  # Accuracy can still be useful to see 
)



# Step 5: Train the Model
history <- model %>% fit(
  input_data_train, output_data_train,
  epochs = 100,
  batch_size = 30,
  validation_split = 0.1
)

model %>% evaluate(input_data_test, output_data_test)



predictions <- predict(model, as.matrix(final_test_set[,-281]))

predictions.df <- as.data.frame(predictions)
predictions.df$num <- 0

# estimate the predictions by using each probability of each category * the mean of that category
for (i in 1:214) {
  for (j in 1:1425) {
    predictions.df[i,1426] <- predictions.df[i,1426] + predictions.df[i,j]*j
  }
}
predictions.df[,1426]
```


#### 3.3.6 Gradient boosting
```{r}

# this model did not work well and need further debug

# Split the data into features and target
X_train <- train_set[, -ncol(train_set)] # all columns except the last one
y_train <- train_set[, ncol(train_set)]  # only the last column


dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = as.matrix(y_train))

# Set parameters for xgboost
params <- list(
  objective = "multi-class",
  eval_metric = "mlogloss",   
  max_depth = 6,              
  eta = 0.3,
  num_class = 438,     
  nthread = 2                  # number of threads to be used
)

# Number of boosting rounds
nrounds <- 100


# Train the model
model <- xgb.train(params= params, data = dtrain, nrounds = 100)
```


#### 3.3.7 Naive Bayes Gaussian 
```{r}
# this model did not work well and need further debug
  outcome_var <- train_set$C281
  independent_vars <- train_set[, !names(train_set) %in% "C281"]
 
   # Scale the independent variables
  independent_vars_scaled <- as.data.frame(scale(independent_vars))
  
  training_data<- cbind(independent_vars_scaled,outcome_var)
  
X_train <- training_data[, -ncol(training_data)]
# The last column is the target variable
y_train <- training_data[, ncol(training_data)]


# Train the model
nb_model <- naiveBayes(X_train, y_train)

predictions <- predict(nb_model, test_set_201[,1:280])


```

#### 3.3.8 Random Forest
Your documentation here
```{r}

X_train <- training_data[, -ncol(training_data)]
# The last column is the target variable
y_train <- training_data[, ncol(training_data)]

set.seed(123)

# Now train the Random Forest model
rf_model <- randomForest(x = X_train, y = y_train, ntree = 286, mtry = sqrt(ncol(X_train)), importance = TRUE)


# Predict using the model
predictions <- predict(rf_model, test_set_201)

num_comments <- as.data.frame(predictions)
num_comments

```


## 3.4 Model evaluation
```{r}
MSE(predictions,test_set_201)
```

## 3.5 Output-as-a-CSV
```{r}

#outputcsv$num_comments <- as.vector(prediction, final_test_set , type = "matrix")
outputcsv$num_comments <- as.vector(predictions)

num_comments <- as.data.frame(predictions)
write.csv(cbind(ID = (0:213), predictions.df$num), "csv_for_submission/032104NeuralNet_categorical.csv", row.names = FALSE)

```








