---
title: "Assignment 1"
author: "Junbin Wu"
date: "2024-03-10"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Content

1. Model functions (in chronological order of exploration)
  1.1 Simple Linear Model
  1.2 All zeros
  1.3 Decision Tree
  1.4 PCA
    1.4.1 PCA original
    1.4.2 PCA with cross validation
  
2. Model Evaluation functions
  2.1 MSE calculation

3. Main functions
  3.1 Library loading
  3.2 Data loading
  3.3 Models Running
    3.3.1 Simple linear model
    3.3.2 All zeros
    3.3.3 Decision tree
      3.3.3.1 Decision tree using the whole set
      3.3.3.2 Decision second try
    3.3.4 PCA
      3.3.4.1 PCA first try
      3.3.4.2 PCA cross validation
      3.3.4.3 PCA third try
  3.4 Model evaluation
  3.5 Output as a CSV

# 1. Model functions (in chronological order)

## 1.1 Simple Linear Model

input a training data set
output fit model
```{r}
sim_liner_mod <- function(train) {
  # use `52` as the independent varaible, and `281` as the dependent variable.
  fit <- lm(`281` ~ `7`+`52`+`61`+`62`, data = train)
  return(fit)
}
```

## 1.2 All zeros
By predicting all outcomes as zeros, the MSE is even better than 2.1 Linear Model, which indicate that a simple linear regression would not work.

## 1.3 Decision tree
input a training data set
output a fit model
```{r}

deci_tree <- function(train) {
  tree <- rpart(`281` ~ ., data = train, method = "anova")
  return(tree)
}

```

## 1.4 PCA

### 1.4.1 PCA original
input two data set, the train and the test
output predictions on the test
```{r}
PCA <- function(train, test) {
  
#####train the model
  # independent_variables
  independent_vars <- train[, !names(train) %in% "281"]
  
  # Scale the independent variables
  independent_vars_scaled <- scale(independent_vars)
  
  # Running PCA on the independent variables
  pca_result <- prcomp(independent_vars_scaled, scale. = TRUE)
  
  # Scree plot to aid in selecting the number of principal components
  #plot(pca_result$sdev^2, type = "b", xlab = "Principal Component", ylab = "Variance Explained")
  
  # calculate the explained variances
  prop_var_explained <- cumsum(pca_result$sdev^2) / sum(pca_result$sdev^2)
  
  # only select whose explained variances larger than 0.95
  n_components <- which.max(prop_var_explained >= 0.95)
  
  pca_scores <- pca_result$x[, 1:n_components]
  
  # Combine PCA scores with the outcome variable
  regression_data <- as.data.frame(cbind(pca_scores, outcome = train$`281`))
  
  # Linear regression
  lm_result <- lm(outcome ~ ., data = regression_data)
  

##### make predictions
  #  test is the new test dataset
  new_independent_vars <- test[, names(test) %in% names(independent_vars)]
  # Scale the test
  new_independent_vars_scaled <- scale(new_independent_vars, center = attr(independent_vars_scaled, "scaled:center"), scale = attr(independent_vars_scaled, "scaled:scale"))
  
  # Transform test data with PCA
  new_pca_scores <- predict(pca_result, newdata = new_independent_vars_scaled)
  
  # Selecting the same number of principal components
  new_pca_scores_selected <- new_pca_scores[, 1:n_components]
  
  # Make predictions
  predictions <- predict(lm_result, newdata = data.frame(new_pca_scores_selected))
  return(predictions)
}
```

### 1.4.2 PCA with cross validation
input training set, seed for randomization, k-fold of cross validation
output the cross validation results
```{r}
PCAcv <- function(train, seed, kf) {
  
  # Assuming 'data' is your dataframe and 'outcome' is the name of your outcome variable
  independent_vars <- train[, !names(train) %in% "281"]
  # Scale the independent variables
  independent_vars_scaled <- scale(independent_vars)
  
  #set cv
  set.seed(seed) # for reproducibility
  outcome_var <- train_set_zeroCOLremoved$`281`
  folds <- createFolds(outcome_var, k = kf, list = TRUE)
  


  # Placeholder for cross-validation results
  cv_results <- data.frame(RMSE = rep(NA, length(folds)))


  # Loop over each fold
  for(i in seq_along(folds)) {
      # Split the data into training and testing sets
      test_indices <-  folds[[i]]
      train_indices <- (1:nrow(train_set_zeroCOLremoved))[-test_indices]
      
      
      # PCA on training data
      pca_result <- prcomp(independent_vars_scaled[train_indices, ], center = TRUE, scale. = TRUE)
      # Determine number of components explaining 95% variance or use another criterion
      prop_var_explained <- cumsum(pca_result$sdev^2) / sum(pca_result$sdev^2)
      num_components <- which.max(prop_var_explained >= 0.95)
      # Prepare the training data with selected components
      pca_train <- predict(pca_result, newdata = independent_vars_scaled[train_indices, ])[, 1:num_components]
      pca_test <- predict(pca_result, newdata = independent_vars_scaled[test_indices, ])[, 1:num_components]
      
      model <- glmnet(pca_train, outcome_var[train_indices], family = "gaussian")
      summary(model)
      # Predict on testing data
      predictions <- predict(model, newx = pca_test)
      
      # Calculate performance metrics
      cv_results$RMSE[i] <- sqrt(mean((predictions - outcome_var[test_indices])^2))
      #cv_results$R2[i] <- cor(predictions, outcome_var[test_indices])^2
  }
  return(cv_results)

}
```

# 2. Model Evaluation functions

## 2.1 MSE calculation
input a vector of prediction value, the test data set
output a mse
```{r}
MSE <- function(prediction, test) {
  mse <- mean((prediction - test$`281`)^2)
  return(mse)
}

```



# 3. Main functions
The Main function calls all other functions above intermittently for the purpose of high efficiency and maintenance, due to high cohesion and low coupling.



## 3.1 Library loading
```{r}
library(readr)
library(glmnet)
library(rpart)
library(rpart.plot)
library(rlang)
library(dplyr)
library(ggplot2)
library(cli)
library(caret)

```



## 3.2 Data loading
The data used here have been pre-processed. I added headers 1-281 for every data set. For the training set, I removed all duplicates, so the total observations come down from 52,397 to 49,203.
```{r message=FALSE, warning=FALSE}
train_set <- read_csv("data/Processed Data Set/blogData_train duplicate removed.csv")

final_test_set <- read_csv("data/Processed Data Set/blogData_test.csv")

test_set_201 <- read_csv("data/Processed Data Set/blogData_test-2012.02.01.00_00.csv")

test_set_202 <- read_csv("data/Processed Data Set/blogData_test-2012.02.02.00_00.csv")

test_set_203 <- read_csv("data/Processed Data Set/blogData_test-2012.02.03.00_00.csv")
```


## 3.3 Models running

### 3.3.1 Simple Linear Model
```{r}
fit <- sim_liner_mod(train_set)
predictions <- predict(fit, final_test_set)
```

### 3.3.2: the results from 3.3.1 are so bad that it is even worse than entering all zeros 

### 3.3.3 Decision tree
#### 3.3.3.1 decision tree first try
```{r}
first_tree <- deci_tree(train_set)
predictions <- predict(first_tree, final_test_set, type = "matrix")
# id with high outcomes 8,20,24,25,30,40,49,71,95,109,111,141,155,166,167,184,203

```

### 3.3.3.2 decision tree second try, training specifically for whose output probably >3 and < 200
```{r}
train_3plus <- train_set[which(train_set$`281`>3 & which(train_set$`281`<200)),]
tree3plus <- deci_tree(train_3plus)

id <- c(8,20,24,25,30,40,49,71,95,109,111,141,155,166,167,184,203)
id <- id + 1
final_test_set3plus <- final_test_set[id,]
predictions <- predict(tree3plus, final_test_set3plus,type = "matrix")
```


### 3.3.4 PCA

#### 3.3.4.1 PCA first try
```{r}
# remove any constant zeros column
train_set_zeroCOLremoved <- train_set[,apply(train_set, 2, function(x) var(x)!=0)]

predictions <- cbind(ID = c(0:213), num_comments = PCA(train_set_zeroCOLremoved, final_test_set))
```


#### 3.3.4.2 PCA second try, using cross-validation
```{r}
train_set_zeroCOLremoved <- train_set[,apply(train_set, 2, function(x) var(x)!=0)]
cvresults <- PCAcv(train_set_zeroCOLremoved, 123, 5)

```


#### 3.3.4.3 PCA third try, identify potential outliers and train a extra model for those outliers ######
```{r}
#train_set_zeroCOLremovedPlus10 <- train_set[,]
# by observing the results from the first PCA try, and the results from the decision tree,
# I discover that the following id might have higher outcome: 8,20,24,25,30,40,49,71,95,109,111,131,141,155,166,167,184,203
# among which, id 131 is extremely high, somewhere around 600.
# for this try in PCA, I try a model specifically for this outliers.

# only use data whose output is between 2 and 200
train_set_zeroCOLremoved <- train_set[which(train_set$`281`> 2 & train_set$`281`< 200),]
train_set_zeroCOLremoved <- train_set_zeroCOLremovedPlus10[,apply(train_set_zeroCOLremovedPlus10, 2, function(x) var(x)!=0)]

# the test set, this id is identified in the first try of PCA.
id <- c(8,20,24,25,30,40,49,71,95,109,111,141,155,166,167,184,203)
id <- id + 1
final_test_set2plus <- final_test_set[id,]

predictions <- cbind(ID = id-1, num_comments = PCA(train_set_zeroCOLremoved,final_test_set2plus))

```
#? Fourth try on PCA #######################################################
# this time train 5 models for different types of outcomes
# type 1: 0-10; type 2: 10-20; type 3: 20-40

##### Random Forest ###########################################################


#### Neural Network ##########################################################


#### Naive Bayes Gaussian #####################################################

##### gradient boosting ########################################################


```

## 3.4 Model evaluation
```{r}
MSE(predictions,test_set_201)
```

## 3.5 Output-as-a-CSV
```{r}

#outputcsv$num_comments <- as.vector(prediction, final_test_set , type = "matrix")
outputcsv$num_comments <- as.vector(predictions)


write.csv(as.vector(predictions), "csv_for_submission/031507_PCAminus200remove131.csv", row.names = FALSE)

```








